2025-05-11 00:47:40,015 - evalscope - INFO - Loading model /home/tlmsq/rlrover/checkpoint-1868 ...
2025-05-11 00:47:55,580 - evalscope - WARNING - Got local model dir: /home/tlmsq/rlrover/checkpoint-1868
2025-05-11 00:47:55,581 - evalscope - INFO - Updating generation config ...
2025-05-11 00:47:55,591 - evalscope - INFO - Dump task config to ./outputs/20250511_004737/configs/task_config_bb92df.yaml
2025-05-11 00:47:55,593 - evalscope - INFO - {
    "model": "/home/tlmsq/rlrover/checkpoint-1868",
    "model_id": "checkpoint-1868",
    "model_args": {
        "revision": "master",
        "precision": "torch.float16"
    },
    "model_task": "text_generation",
    "template_type": null,
    "chat_template": null,
    "datasets": [
        "mmlu"
    ],
    "dataset_args": {
        "mmlu": {
            "name": "mmlu",
            "dataset_id": "modelscope/mmlu",
            "model_adapter": "generation",
            "output_types": [
                "multiple_choice_logits",
                "generation"
            ],
            "subset_list": [
                "high_school_european_history",
                "business_ethics",
                "clinical_knowledge",
                "medical_genetics",
                "high_school_us_history",
                "high_school_physics",
                "high_school_world_history",
                "virology",
                "high_school_microeconomics",
                "econometrics",
                "college_computer_science",
                "high_school_biology",
                "abstract_algebra",
                "professional_accounting",
                "philosophy",
                "professional_medicine",
                "nutrition",
                "global_facts",
                "machine_learning",
                "security_studies",
                "public_relations",
                "professional_psychology",
                "prehistory",
                "anatomy",
                "human_sexuality",
                "college_medicine",
                "high_school_government_and_politics",
                "college_chemistry",
                "logical_fallacies",
                "high_school_geography",
                "elementary_mathematics",
                "human_aging",
                "college_mathematics",
                "high_school_psychology",
                "formal_logic",
                "high_school_statistics",
                "international_law",
                "high_school_mathematics",
                "high_school_computer_science",
                "conceptual_physics",
                "miscellaneous",
                "high_school_chemistry",
                "marketing",
                "professional_law",
                "management",
                "college_physics",
                "jurisprudence",
                "world_religions",
                "sociology",
                "us_foreign_policy",
                "high_school_macroeconomics",
                "computer_security",
                "moral_scenarios",
                "moral_disputes",
                "electrical_engineering",
                "astronomy",
                "college_biology"
            ],
            "metric_list": [
                "AverageAccuracy"
            ],
            "few_shot_num": 5,
            "few_shot_random": false,
            "train_split": "train",
            "eval_split": "test",
            "prompt_template": "Answer the following multiple choice question about {subset_name}. The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.\n\n{query}",
            "system_prompt": null,
            "query_template": null,
            "pretty_name": "MMLU",
            "filters": null,
            "extra_params": {}
        }
    },
    "dataset_dir": "/home/tlmsq/.cache/modelscope/hub/datasets",
    "dataset_hub": "modelscope",
    "generation_config": {
        "max_length": 2048,
        "max_new_tokens": 512,
        "do_sample": false,
        "top_k": 50,
        "top_p": 1.0,
        "temperature": 1.0
    },
    "eval_type": "checkpoint",
    "eval_backend": "Native",
    "eval_config": null,
    "stage": "all",
    "limit": 5,
    "eval_batch_size": 1,
    "mem_cache": false,
    "use_cache": null,
    "work_dir": "./outputs/20250511_004737",
    "outputs": null,
    "debug": false,
    "dry_run": false,
    "seed": 42,
    "api_url": null,
    "api_key": "EMPTY",
    "timeout": null,
    "stream": false,
    "judge_strategy": "auto",
    "judge_worker_num": 1,
    "judge_model_args": {}
}
2025-05-11 00:47:55,594 - evalscope - INFO - **** Start evaluating on dataset modelscope/mmlu ****
2025-05-11 00:47:55,594 - evalscope - INFO - Loading dataset from hub: modelscope/mmlu
2025-05-11 00:47:55,844 - evalscope - INFO - Loading dataset: dataset_name: modelscope/mmlu > subsets: ['high_school_european_history', 'business_ethics', 'clinical_knowledge', 'medical_genetics', 'high_school_us_history', 'high_school_physics', 'high_school_world_history', 'virology', 'high_school_microeconomics', 'econometrics', 'college_computer_science', 'high_school_biology', 'abstract_algebra', 'professional_accounting', 'philosophy', 'professional_medicine', 'nutrition', 'global_facts', 'machine_learning', 'security_studies', 'public_relations', 'professional_psychology', 'prehistory', 'anatomy', 'human_sexuality', 'college_medicine', 'high_school_government_and_politics', 'college_chemistry', 'logical_fallacies', 'high_school_geography', 'elementary_mathematics', 'human_aging', 'college_mathematics', 'high_school_psychology', 'formal_logic', 'high_school_statistics', 'international_law', 'high_school_mathematics', 'high_school_computer_science', 'conceptual_physics', 'miscellaneous', 'high_school_chemistry', 'marketing', 'professional_law', 'management', 'college_physics', 'jurisprudence', 'world_religions', 'sociology', 'us_foreign_policy', 'high_school_macroeconomics', 'computer_security', 'moral_scenarios', 'moral_disputes', 'electrical_engineering', 'astronomy', 'college_biology']
2025-05-11 01:01:14,262 - evalscope - INFO - Use settings: > few_shot_num: 5, > few_shot_split: train, > target_eval_split: test
2025-05-11 01:02:10,127 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_high_school_european_history.jsonl.
2025-05-11 01:02:23,445 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_business_ethics.jsonl.
2025-05-11 01:02:40,939 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_clinical_knowledge.jsonl.
2025-05-11 01:03:14,141 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_medical_genetics.jsonl.
2025-05-11 01:04:03,286 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_high_school_us_history.jsonl.
2025-05-11 01:04:36,268 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_high_school_physics.jsonl.
2025-05-11 01:05:26,782 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_high_school_world_history.jsonl.
2025-05-11 01:05:58,791 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_virology.jsonl.
2025-05-11 01:06:56,551 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_high_school_microeconomics.jsonl.
2025-05-11 01:07:55,466 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_econometrics.jsonl.
2025-05-11 01:08:49,243 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_college_computer_science.jsonl.
2025-05-11 01:09:47,153 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_high_school_biology.jsonl.
2025-05-11 01:10:46,968 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_abstract_algebra.jsonl.
2025-05-11 01:11:28,219 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_professional_accounting.jsonl.
2025-05-11 01:11:43,251 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_philosophy.jsonl.
2025-05-11 01:12:26,178 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_professional_medicine.jsonl.
2025-05-11 01:13:19,079 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_nutrition.jsonl.
2025-05-11 01:13:19,632 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_global_facts.jsonl.
2025-05-11 01:14:07,391 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_machine_learning.jsonl.
2025-05-11 01:14:55,896 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_security_studies.jsonl.
2025-05-11 01:15:12,774 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_public_relations.jsonl.
2025-05-11 01:16:10,978 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_professional_psychology.jsonl.
2025-05-11 01:16:29,749 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_prehistory.jsonl.
2025-05-11 01:16:51,902 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_anatomy.jsonl.
2025-05-11 01:17:04,771 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_human_sexuality.jsonl.
2025-05-11 01:17:45,463 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_college_medicine.jsonl.
2025-05-11 01:18:31,625 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_high_school_government_and_politics.jsonl.
2025-05-11 01:18:57,499 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_college_chemistry.jsonl.
2025-05-11 01:19:49,908 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_logical_fallacies.jsonl.
2025-05-11 01:20:08,655 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_high_school_geography.jsonl.
2025-05-11 01:20:57,492 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_elementary_mathematics.jsonl.
2025-05-11 01:21:24,108 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_human_aging.jsonl.
2025-05-11 01:22:20,403 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_college_mathematics.jsonl.
2025-05-11 01:23:06,362 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_high_school_psychology.jsonl.
2025-05-11 01:23:53,807 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_formal_logic.jsonl.
2025-05-11 01:24:53,954 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_high_school_statistics.jsonl.
2025-05-11 01:25:17,135 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_international_law.jsonl.
2025-05-11 01:25:36,174 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_high_school_mathematics.jsonl.
2025-05-11 01:26:12,984 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_high_school_computer_science.jsonl.
2025-05-11 01:27:03,021 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_conceptual_physics.jsonl.
2025-05-11 01:27:14,104 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_miscellaneous.jsonl.
2025-05-11 01:27:27,491 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_high_school_chemistry.jsonl.
2025-05-11 01:27:56,358 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_marketing.jsonl.
2025-05-11 01:28:46,035 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_professional_law.jsonl.
2025-05-11 01:28:59,873 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_management.jsonl.
2025-05-11 01:29:36,420 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_college_physics.jsonl.
2025-05-11 01:29:47,841 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_jurisprudence.jsonl.
2025-05-11 01:29:55,156 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_world_religions.jsonl.
2025-05-11 01:30:12,223 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_sociology.jsonl.
2025-05-11 01:30:39,535 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_us_foreign_policy.jsonl.
2025-05-11 01:31:24,408 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_high_school_macroeconomics.jsonl.
2025-05-11 01:31:58,426 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_computer_security.jsonl.
2025-05-11 01:31:59,876 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_moral_scenarios.jsonl.
2025-05-11 01:32:36,944 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_moral_disputes.jsonl.
2025-05-11 01:33:28,279 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_electrical_engineering.jsonl.
2025-05-11 01:34:15,563 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_astronomy.jsonl.
2025-05-11 01:34:31,312 - evalscope - INFO - Dump predictions to ./outputs/20250511_004737/predictions/checkpoint-1868/mmlu_college_biology.jsonl.
2025-05-11 01:34:31,370 - evalscope - INFO - Dump report: ./outputs/20250511_004737/reports/checkpoint-1868/mmlu.json 

2025-05-11 01:34:31,395 - evalscope - INFO - Report table: 
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| Model           | Dataset   | Metric          | Subset                              |   Num |   Score | Cat.0          |
+=================+===========+=================+=====================================+=======+=========+================+
| checkpoint-1868 | mmlu      | AverageAccuracy | high_school_european_history        |     5 |     0   | Humanities     |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | high_school_us_history              |     5 |     0.2 | Humanities     |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | high_school_world_history           |     5 |     0.2 | Humanities     |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | philosophy                          |     5 |     0.8 | Humanities     |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | prehistory                          |     5 |     0.2 | Humanities     |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | logical_fallacies                   |     5 |     0.2 | Humanities     |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | formal_logic                        |     5 |     0.4 | Humanities     |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | international_law                   |     5 |     0.6 | Humanities     |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | professional_law                    |     5 |     0.4 | Humanities     |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | jurisprudence                       |     5 |     0.4 | Humanities     |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | world_religions                     |     5 |     0.8 | Humanities     |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | moral_scenarios                     |     5 |     0.2 | Humanities     |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | moral_disputes                      |     5 |     0.4 | Humanities     |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | business_ethics                     |     5 |     0.6 | Other          |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | clinical_knowledge                  |     5 |     0.4 | Other          |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | medical_genetics                    |     5 |     0.2 | Other          |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | virology                            |     5 |     0   | Other          |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | professional_accounting             |     5 |     0.4 | Other          |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | professional_medicine               |     5 |     0.2 | Other          |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | nutrition                           |     5 |     0.6 | Other          |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | global_facts                        |     5 |     0.2 | Other          |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | anatomy                             |     5 |     0.4 | Other          |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | college_medicine                    |     5 |     0.6 | Other          |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | human_aging                         |     5 |     0.4 | Other          |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | miscellaneous                       |     5 |     0.4 | Other          |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | marketing                           |     5 |     0.2 | Other          |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | management                          |     5 |     0.4 | Other          |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | high_school_physics                 |     5 |     0.4 | STEM           |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | college_computer_science            |     5 |     0   | STEM           |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | high_school_biology                 |     5 |     0.6 | STEM           |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | abstract_algebra                    |     5 |     0.2 | STEM           |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | machine_learning                    |     5 |     0.2 | STEM           |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | college_chemistry                   |     5 |     0.8 | STEM           |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | elementary_mathematics              |     5 |     0.2 | STEM           |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | college_mathematics                 |     5 |     0.6 | STEM           |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | high_school_statistics              |     5 |     0.8 | STEM           |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | high_school_mathematics             |     5 |     0   | STEM           |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | high_school_computer_science        |     5 |     0.2 | STEM           |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | conceptual_physics                  |     5 |     0.6 | STEM           |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | high_school_chemistry               |     5 |     0.4 | STEM           |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | college_physics                     |     5 |     0.2 | STEM           |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | computer_security                   |     5 |     0.4 | STEM           |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | electrical_engineering              |     5 |     0.2 | STEM           |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | astronomy                           |     5 |     0.6 | STEM           |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | college_biology                     |     5 |     0   | STEM           |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | high_school_microeconomics          |     5 |     1   | Social Science |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | econometrics                        |     5 |     0.4 | Social Science |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | security_studies                    |     5 |     0.2 | Social Science |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | public_relations                    |     5 |     0.2 | Social Science |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | professional_psychology             |     5 |     0.2 | Social Science |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | human_sexuality                     |     5 |     0.6 | Social Science |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | high_school_government_and_politics |     5 |     0.2 | Social Science |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | high_school_geography               |     5 |     0.4 | Social Science |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | high_school_psychology              |     5 |     0.6 | Social Science |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | sociology                           |     5 |     0.4 | Social Science |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | us_foreign_policy                   |     5 |     0.4 | Social Science |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+
| checkpoint-1868 | mmlu      | AverageAccuracy | high_school_macroeconomics          |     5 |     0.2 | Social Science |
+-----------------+-----------+-----------------+-------------------------------------+-------+---------+----------------+ 

2025-05-11 01:34:31,396 - evalscope - INFO - **** Evaluation finished on modelscope/mmlu ****

